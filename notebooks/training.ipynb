{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Music Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001 \n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory created at: ..\\runs\\20251202_142134\n"
     ]
    }
   ],
   "source": [
    "# Setup run directory\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = Path(f\"../runs/{run_id}\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Run directory created at: {run_dir}\")\n",
    "\n",
    "changes_file = run_dir / \"changes.md\"\n",
    "with open(changes_file, \"w\") as f:\n",
    "    f.write(f\"# Run {run_id}\\n\\n\")\n",
    "    f.write(\"## Configuration\\n\")\n",
    "    f.write(f\"- Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"- Learning Rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"- Epochs: {NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"- Device: {device}\\n\")\n",
    "    f.write(f\"- Data Strategy: Chunking (3s chunks, 50% overlap)\\n\")\n",
    "    f.write(f\"- Augmentation: Noise=0.01, Shift=0.3\\n\")\n",
    "    f.write(f\"- Optimization: In-memory caching + Mixed Precision (AMP)\\n\")\n",
    "    f.write(f\"- Stability: Seed=42, Weight Decay=1e-4 (Standard), Gradient Clipping=1.0\\n\")\n",
    "    f.write(f\"- Data Split: Stratified (Balanced Validation Set)\\n\\n\")\n",
    "    f.write(\"## Changes\\n\")\n",
    "    f.write(\"- Increased Dropout in Residual Blocks from 0.2 to 0.3 to combat slight overfitting.\\n\\n\")\n",
    "    f.write(\"## Results\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Single-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.4, device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # pass with mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        \n",
    "        # Backwardprop and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Weighted accuracy for mixup\n",
    "        correct += (lam * (predicted == targets_a).float() + (1 - lam) * (predicted == targets_b).float()).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, \n",
    "                save_path='../models/best_model.pth', changes_file=None):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # OneCycleLR Scheduler\n",
    "    # Steps per epoch is len(train_loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate, \n",
    "        steps_per_epoch=len(train_loader), \n",
    "        epochs=num_epochs,\n",
    "        pct_start=0.3, # Warmup for 30% of training\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=1000.0\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Apply Mixup\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.4, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Step scheduler every batch for OneCycleLR\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (lam * (predicted == targets_a).float() + (1 - lam) * (predicted == targets_b).float()).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # saving best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Val Loss: {history['val_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Train Acc: {history['train_acc'][-1]:.2f}%\\n\")\n",
    "            f.write(f\"- Final Val Acc: {history['val_acc'][-1]:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, multi_label=False, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 2 if not multi_label else 1, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Accuracy')\n",
    "    axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Single-label Classification (GTZAN, FMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching 999 audio files to memory...\n",
      "Caching complete.\n",
      "GTZAN files: 999\n",
      "Created stratified split: 719 train, 180 val, 100 test songs\n",
      "Applying chunking: 3.0s chunks with 50% overlap\n",
      "Chunked dataset sizes: 13661 train, 3420 val, 1900 test chunks\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 47/427 [00:08<01:10,  5.40it/s, loss=2.15, lr=4.01e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     24\u001b[39m train_loader, val_loader, test_loader = create_dataloaders(\n\u001b[32m     25\u001b[39m     dataset, \n\u001b[32m     26\u001b[39m     batch_size=BATCH_SIZE, \n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     test_split=\u001b[32m0.1\u001b[39m \u001b[38;5;66;03m# Create test split\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m model = ComplexCNN(n_classes=\u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgtzan_cnn.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchanges_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchanges_file\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m plot_training_history(history, save_path=\u001b[38;5;28mstr\u001b[39m(run_dir / \u001b[33m'\u001b[39m\u001b[33mtraining_history.png\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgtzan_cnn.pth\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate, device, save_path, changes_file)\u001b[39m\n\u001b[32m     40\u001b[39m scaler = torch.amp.GradScaler()\n\u001b[32m     42\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Apply Mixup\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\utils\\datasets_gtzan.py:192\u001b[39m, in \u001b[36mChunkedDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    189\u001b[39m     chunk = torch.nn.functional.pad(chunk, (\u001b[32m0\u001b[39m, padding))\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chunk, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\utils\\datasets_gtzan.py:211\u001b[39m, in \u001b[36mAudioAugmentation.__call__\u001b[39m\u001b[34m(self, waveform)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Time shift (roll)\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shift_max > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     shift_amt = \u001b[38;5;28mint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.shift_max * waveform.shape[\u001b[32m1\u001b[39m])\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.rand(\u001b[32m1\u001b[39m).item() > \u001b[32m0.5\u001b[39m:\n\u001b[32m    213\u001b[39m         shift_amt = -shift_amt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "\n",
    "%run \"./model_cnn.ipynb\"\n",
    "\n",
    "from utils.datasets_gtzan import GTZANDataset, create_dataloaders, GENRES, AudioAugmentation\n",
    "\n",
    "# Create dataset with in-memory caching\n",
    "gtzan_root = repo_root / \"data\" / \"gtzan\"\n",
    "dataset = GTZANDataset(str(gtzan_root), cache_to_memory=True)\n",
    "print(f\"GTZAN files: {len(dataset)}\")\n",
    "\n",
    "# Augment data\n",
    "train_transform = AudioAugmentation(noise_level=0.01, shift_max=0.3)\n",
    "\n",
    "# Create loaders with Stratified Split and Chunking\n",
    "# NOTE: With cache_to_memory=True, use num_workers=0 on Windows to avoid \n",
    "# pickling the entire cached dataset to worker processes, which causes hangs/OOM.\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    train_transform=train_transform,\n",
    "    chunk_length_sec=3.0, # Enable chunking\n",
    "    test_split=0.1 # Create test split\n",
    ")\n",
    "\n",
    "model = ComplexCNN(n_classes=10)\n",
    "\n",
    "history = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device,\n",
    "    save_path=str(run_dir / 'gtzan_cnn.pth'),\n",
    "    changes_file=changes_file\n",
    ")\n",
    "\n",
    "plot_training_history(history, save_path=str(run_dir / 'training_history.png'))\n",
    "\n",
    "print(f\"Loading best model from {run_dir / 'gtzan_cnn.pth'}...\")\n",
    "model.load_state_dict(torch.load(str(run_dir / 'gtzan_cnn.pth')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 107/107 [00:03<00:00, 33.23it/s]\n",
      "Evaluating Validation: 100%|██████████| 107/107 [00:03<00:00, 33.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics (Chunk-Level):\n",
      "Accuracy: 80.23%\n",
      "Precision: 0.8043\n",
      "Recall: 0.8023\n",
      "F1-Score: 0.7947\n",
      "\n",
      "Evaluating on 180 songs (Validation) (aggregating 19 chunks each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Song Eval: 100%|██████████| 180/180 [00:03<00:00, 57.61it/s]\n",
      "Song Eval: 100%|██████████| 180/180 [00:03<00:00, 57.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Song-Level Accuracy: 85.56%\n",
      "\n",
      "--- Test Set Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Test: 100%|██████████| 60/60 [00:02<00:00, 20.69it/s]\n",
      "Evaluating Test: 100%|██████████| 60/60 [00:02<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics (Chunk-Level):\n",
      "Accuracy: 77.16%\n",
      "Precision: 0.7823\n",
      "Recall: 0.7716\n",
      "F1-Score: 0.7657\n",
      "\n",
      "Evaluating on 100 songs (Test) (aggregating 19 chunks each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Song Eval: 100%|██████████| 100/100 [00:01<00:00, 60.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Song-Level Accuracy: 83.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device, genre_names=None, changes_file=None, split_name=\"Test\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f'Evaluating {split_name}'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{split_name} Metrics (Chunk-Level):\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- {split_name} Accuracy (Chunk): {accuracy*100:.2f}%\\n\")\n",
    "            f.write(f\"- {split_name} Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"- {split_name} Recall: {recall:.4f}\\n\")\n",
    "            f.write(f\"- {split_name} F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def evaluate_by_song(model, val_dataset, device, changes_file=None, split_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy by aggregating chunk predictions for each song.\n",
    "    Val_dataset is ordered by song.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_songs = 0\n",
    "    \n",
    "    if not hasattr(val_dataset, 'num_chunks'):\n",
    "        print(\"Dataset does not appear to be a ChunkedDataset. Skipping song-level evaluation.\")\n",
    "        return 0.0\n",
    "\n",
    "    num_chunks = val_dataset.num_chunks\n",
    "    total_songs = len(val_dataset) // num_chunks\n",
    "    \n",
    "    print(f\"\\nEvaluating on {total_songs} songs ({split_name}) (aggregating {num_chunks} chunks each)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(total_songs), desc='Song Eval'):\n",
    "            chunks = []\n",
    "            label = None\n",
    "            \n",
    "            start_idx = i * num_chunks\n",
    "            \n",
    "            for j in range(num_chunks):\n",
    "                c, l = val_dataset[start_idx + j]\n",
    "                chunks.append(c)\n",
    "                label = l \n",
    "            \n",
    "            chunks_tensor = torch.stack(chunks).to(device)\n",
    "            \n",
    "            outputs = model(chunks_tensor) # (num_chunks, n_classes)\n",
    "            \n",
    "            avg_output = torch.mean(outputs, dim=0)\n",
    "            pred_label = torch.argmax(avg_output).item()\n",
    "            \n",
    "            if pred_label == label:\n",
    "                correct_songs += 1\n",
    "                \n",
    "    song_acc = 100 * correct_songs / total_songs\n",
    "    print(f\"{split_name} Song-Level Accuracy: {song_acc:.2f}%\")\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- {split_name} Song-Level Accuracy: {song_acc:.2f}%\\n\")\n",
    "            \n",
    "    return song_acc\n",
    "\n",
    "with open(changes_file, \"a\") as f:\n",
    "    f.write(\"\\n--- Validation Set ---\\n\")\n",
    "print(\"\\n--- Validation Set Evaluation ---\")\n",
    "evaluate_model(\n",
    "    model, val_loader, device, genre_names=GENRES, changes_file=changes_file, split_name=\"Validation\"\n",
    ")\n",
    "evaluate_by_song(model, val_loader.dataset, device, changes_file=changes_file, split_name=\"Validation\")\n",
    "\n",
    "with open(changes_file, \"a\") as f:\n",
    "    f.write(\"\\n--- Test Set ---\\n\")\n",
    "print(\"\\n--- Test Set Evaluation ---\")\n",
    "evaluate_model(\n",
    "    model, test_loader, device, genre_names=GENRES, changes_file=changes_file, split_name=\"Test\"\n",
    ")\n",
    "\n",
    "evaluate_by_song(model, test_loader.dataset, device, changes_file=changes_file, split_name=\"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tagging-Music-Sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
