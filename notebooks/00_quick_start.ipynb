{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "This notebook provides a quick demonstration of the music classification pipeline without requiring actual datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check required packages\n",
    "required_packages = ['torch', 'torchaudio', 'numpy', 'matplotlib', 'librosa', 'sklearn']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ“ {package} is installed\")\n",
    "    except ImportError:\n",
    "        print(f\"âœ— {package} is NOT installed\")\n",
    "        print(f\"   Install with: pip install {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Demo\n",
    "\n",
    "Let's create and test model architectures with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for music genre classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=10, sample_rate=22050, n_mels=128):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        # Mel-spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convert to mel-spectrogram\n",
    "        x = self.mel_spec(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global pooling and flatten\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = SimpleCNN(n_classes=10)\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model with Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic audio (random noise)\n",
    "batch_size = 4\n",
    "sample_rate = 22050\n",
    "duration = 30  # seconds\n",
    "n_samples = sample_rate * duration\n",
    "\n",
    "# Generate random audio\n",
    "synthetic_audio = torch.randn(batch_size, 1, n_samples)\n",
    "\n",
    "print(f\"Synthetic audio shape: {synthetic_audio.shape}\")\n",
    "print(f\"Audio duration: {duration} seconds\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(synthetic_audio)\n",
    "\n",
    "print(f\"Input shape: {synthetic_audio.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput logits (first sample): {output[0]}\")\n",
    "\n",
    "# Convert to probabilities\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "print(f\"\\nProbabilities (first sample): {probabilities[0]}\")\n",
    "print(f\"Sum of probabilities: {probabilities[0].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mel-spectrogram for visualization\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "# Take first sample\n",
    "audio_sample = synthetic_audio[0]\n",
    "mel_spec = mel_transform(audio_sample)\n",
    "mel_spec_db = amplitude_to_db(mel_spec)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Waveform\n",
    "axes[0].plot(audio_sample[0].numpy())\n",
    "axes[0].set_title('Waveform (Synthetic Audio)', fontsize=14)\n",
    "axes[0].set_xlabel('Sample')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mel-spectrogram\n",
    "im = axes[1].imshow(mel_spec_db[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Mel-Spectrogram (dB)', fontsize=14)\n",
    "axes[1].set_xlabel('Time Frames')\n",
    "axes[1].set_ylabel('Mel Frequency Bins')\n",
    "plt.colorbar(im, ax=axes[1], label='Amplitude (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTZAN genre names\n",
    "genre_names = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "               'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# Get predictions for first sample\n",
    "probs = probabilities[0].numpy()\n",
    "predicted_idx = np.argmax(probs)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(genre_names, probs, color='steelblue')\n",
    "\n",
    "# Highlight predicted genre\n",
    "bars[predicted_idx].set_color('coral')\n",
    "\n",
    "ax.set_xlabel('Probability', fontsize=12)\n",
    "ax.set_title('Genre Prediction Probabilities', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (name, prob) in enumerate(zip(genre_names, probs)):\n",
    "    ax.text(prob + 0.01, i, f'{prob:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPredicted genre: {genre_names[predicted_idx]}\")\n",
    "print(f\"Confidence: {probs[predicted_idx]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count model parameters by layer.\"\"\"\n",
    "    total_params = 0\n",
    "    print(\"Layer-wise Parameter Count:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        total_params += params\n",
    "        print(f\"{name:40s} {params:>15,}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Total Parameters':40s} {total_params:>15,}\")\n",
    "    print(f\"{'Model Size (MB)':40s} {total_params * 4 / (1024**2):>15.2f}\")\n",
    "    \n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've verified the installation and tested the model:\n",
    "\n",
    "1. **Download datasets**: See README.md for dataset download instructions\n",
    "2. **Load real data**: Use notebooks 01-03 to load GTZAN, MTAT, or FMA datasets\n",
    "3. **Train models**: Use notebook 06 to train on real music data\n",
    "4. **Evaluate**: Use notebook 07 for inference on new audio files\n",
    "\n",
    "### Recommended Order:\n",
    "1. `01_data_loading_gtzan.ipynb` - Start with GTZAN (smallest dataset)\n",
    "2. `04_model_cnn.ipynb` - Explore CNN architectures\n",
    "3. `05_model_rnn.ipynb` - Explore RNN architectures\n",
    "4. `06_training.ipynb` - Train your first model\n",
    "5. `07_inference.ipynb` - Make predictions\n",
    "\n",
    "Happy music classification! ðŸŽµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
