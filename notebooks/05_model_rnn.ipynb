{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM Model Architecture for Music Tagging\n",
    "\n",
    "This notebook implements Recurrent Neural Network (LSTM/GRU) architectures for music classification and tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model for Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based model for music genre classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=10, sample_rate=22050, n_mels=128, \n",
    "                 hidden_size=256, num_layers=2, bidirectional=True):\n",
    "        super(MusicLSTM, self).__init__()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Mel-spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_mels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, time)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        x = self.mel_spec(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        # x: (batch, 1, n_mels, time)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time, n_mels)\n",
    "        x = x.squeeze(1).transpose(1, 2)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use the last output\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward hidden states\n",
    "            last_output = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            last_output = h_n[-1]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(last_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model for Music Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGRU(nn.Module):\n",
    "    \"\"\"GRU-based model for music tagging (multi-label).\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=50, sample_rate=22050, n_mels=128,\n",
    "                 hidden_size=256, num_layers=2, bidirectional=True):\n",
    "        super(MusicGRU, self).__init__()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Mel-spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=n_mels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        gru_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc1 = nn.Linear(gru_output_size, 256)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, time)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        x = self.mel_spec(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        # x: (batch, 1, n_mels, time)\n",
    "        \n",
    "        # Reshape for GRU: (batch, time, n_mels)\n",
    "        x = x.squeeze(1).transpose(1, 2)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        \n",
    "        # Use the last output\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward hidden states\n",
    "            last_output = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            last_output = h_n[-1]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(last_output))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    \"\"\"Hybrid CNN-LSTM model for music classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=10, sample_rate=22050, n_mels=128,\n",
    "                 hidden_size=256, num_layers=2):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        # Mel-spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # Calculate feature dimension after CNN\n",
    "        # After 3 pooling layers: n_mels // 8\n",
    "        cnn_output_dim = 128 * (n_mels // 8)\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, time)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        x = self.mel_spec(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        # x: (batch, 1, n_mels, time)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # x: (batch, 128, n_mels//8, time//8)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, time, features)\n",
    "        batch_size, channels, freq, time = x.size()\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, time, channels, freq)\n",
    "        x = x.reshape(batch_size, time, channels * freq)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use last hidden state (bidirectional)\n",
    "        last_output = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(last_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MusicLSTM\n",
    "model_lstm = MusicLSTM(n_classes=10)\n",
    "print(\"MusicLSTM:\")\n",
    "print(model_lstm)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model_lstm.parameters())}\")\n",
    "\n",
    "# Test with random input\n",
    "batch_size = 4\n",
    "sample_rate = 22050\n",
    "duration = 30\n",
    "x = torch.randn(batch_size, 1, sample_rate * duration)\n",
    "output = model_lstm(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MusicGRU\n",
    "model_gru = MusicGRU(n_classes=50)\n",
    "print(\"MusicGRU:\")\n",
    "print(model_gru)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model_gru.parameters())}\")\n",
    "\n",
    "# Test with random input\n",
    "output = model_gru(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CNN-LSTM\n",
    "model_hybrid = CNNLSTM(n_classes=10)\n",
    "print(\"CNN-LSTM:\")\n",
    "print(model_hybrid)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model_hybrid.parameters())}\")\n",
    "\n",
    "# Test with random input\n",
    "output = model_hybrid(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"Save model state dict.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    \"\"\"Load model state dict.\"\"\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# save_model(model_lstm, '../models/music_lstm.pth')\n",
    "# model_lstm = load_model(MusicLSTM(n_classes=10), '../models/music_lstm.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
