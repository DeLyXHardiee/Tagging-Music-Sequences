{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMA (Free Music Archive) Dataset Loading and Preprocessing\n",
    "\n",
    "This notebook demonstrates how to load and preprocess the FMA dataset (advanced).\n",
    "\n",
    "**FMA Dataset**: Large-scale dataset with tracks of various durations and 161 genres.\n",
    "- FMA Small: 8,000 tracks of 30s, 8 balanced genres\n",
    "- FMA Medium: 25,000 tracks of 30s, 16 unbalanced genres\n",
    "- FMA Large: 106,574 tracks of 30s, 161 unbalanced genres\n",
    "- FMA Full: 106,574 untrimmed tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "FMA_AUDIO_PATH = '../data/fma_small'  # or fma_medium, fma_large\n",
    "FMA_METADATA_PATH = '../data/fma_metadata'\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30  # seconds\n",
    "N_MELS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for FMA Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fma_metadata(metadata_path):\n",
    "    \"\"\"Load FMA metadata files.\"\"\"\n",
    "    tracks_file = os.path.join(metadata_path, 'tracks.csv')\n",
    "    genres_file = os.path.join(metadata_path, 'genres.csv')\n",
    "    \n",
    "    if not os.path.exists(tracks_file):\n",
    "        print(f\"Warning: tracks.csv not found at {tracks_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load tracks metadata\n",
    "    tracks = pd.read_csv(tracks_file, index_col=0, header=[0, 1])\n",
    "    \n",
    "    # Load genres\n",
    "    if os.path.exists(genres_file):\n",
    "        genres = pd.read_csv(genres_file, index_col=0)\n",
    "    else:\n",
    "        genres = None\n",
    "    \n",
    "    return tracks, genres\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    \"\"\"Get the file path for a track ID.\"\"\"\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMA Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMADataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for FMA music genre classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir, metadata_path, sample_rate=22050, \n",
    "                 duration=30, transform=None, subset='small'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_dir (string): Directory with audio files\n",
    "            metadata_path (string): Path to metadata directory\n",
    "            sample_rate (int): Target sample rate\n",
    "            duration (int): Duration in seconds\n",
    "            transform (callable, optional): Optional transform\n",
    "            subset (string): Dataset subset ('small', 'medium', 'large')\n",
    "        \"\"\"\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        \n",
    "        # Load metadata\n",
    "        self.tracks, self.genres = load_fma_metadata(metadata_path)\n",
    "        \n",
    "        if self.tracks is not None:\n",
    "            # Filter by subset\n",
    "            if subset in ['small', 'medium', 'large']:\n",
    "                self.tracks = self.tracks[self.tracks['set', 'subset'] <= subset]\n",
    "            \n",
    "            # Get genre information (top-level genre)\n",
    "            self.track_ids = self.tracks.index.tolist()\n",
    "            self.genre_ids = self.tracks['track', 'genre_top'].values\n",
    "            \n",
    "            # Create genre mapping\n",
    "            unique_genres = sorted(set(self.genre_ids))\n",
    "            self.genre_to_idx = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
    "            self.idx_to_genre = {idx: genre for genre, idx in self.genre_to_idx.items()}\n",
    "            \n",
    "            print(f\"FMA {subset} dataset loaded: {len(self.track_ids)} tracks\")\n",
    "            print(f\"Genres: {list(self.genre_to_idx.keys())}\")\n",
    "        else:\n",
    "            self.track_ids = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.track_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        track_id = self.track_ids[idx]\n",
    "        audio_path = get_audio_path(self.audio_dir, track_id)\n",
    "        \n",
    "        # Get genre label\n",
    "        genre = self.genre_ids[idx]\n",
    "        label = self.genre_to_idx[genre]\n",
    "        \n",
    "        # Load audio (FMA uses MP3 format)\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "        except:\n",
    "            # If torchaudio fails with MP3, use librosa as fallback\n",
    "            waveform_np, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
    "            waveform = torch.from_numpy(waveform_np).unsqueeze(0)\n",
    "            sr = self.sample_rate\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Pad or truncate\n",
    "        target_length = self.sample_rate * self.duration\n",
    "        if waveform.shape[1] > target_length:\n",
    "            waveform = waveform[:, :target_length]\n",
    "        elif waveform.shape[1] < target_length:\n",
    "            padding = target_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instance\n",
    "# Note: Uncomment and update paths to use\n",
    "# dataset = FMADataset(\n",
    "#     audio_dir=FMA_AUDIO_PATH,\n",
    "#     metadata_path=FMA_METADATA_PATH,\n",
    "#     sample_rate=SAMPLE_RATE,\n",
    "#     duration=DURATION,\n",
    "#     subset='small'\n",
    "# )\n",
    "# print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fma_sample(dataset, idx=0):\n",
    "    \"\"\"Visualize a sample from FMA dataset.\"\"\"\n",
    "    waveform, label = dataset[idx]\n",
    "    genre = dataset.idx_to_genre[label]\n",
    "    \n",
    "    # Create mel-spectrogram\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=dataset.sample_rate, n_mels=N_MELS\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot waveform\n",
    "    axes[0].plot(waveform[0].numpy())\n",
    "    axes[0].set_title(f'Waveform - Genre: {genre}')\n",
    "    axes[0].set_xlabel('Sample')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot mel-spectrogram\n",
    "    im = axes[1].imshow(mel_spec_db[0].numpy(), aspect='auto', origin='lower')\n",
    "    axes[1].set_title('Mel-Spectrogram (dB)')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Mel Frequency')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# visualize_fma_sample(dataset, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset, batch_size=16, train_split=0.8):\n",
    "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage:\n",
    "# train_loader, val_loader = create_dataloaders(dataset, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
