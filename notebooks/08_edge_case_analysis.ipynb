{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d46dd0",
   "metadata": {},
   "source": [
    "# Edge Case Analysis: Short Sequences & Difficult Genres\n",
    "\n",
    "This notebook analyzes the performance of the trained music genre classification model on edge cases.\n",
    "We specifically investigate:\n",
    "1.  **Short Input Sequences**: How does the model perform when the audio clip is shorter than the training duration (3s)?\n",
    "2.  **Difficult Genres**: Which genres are hardest to classify, and what are the characteristics of misclassified samples?\n",
    "3.  **Confidence Analysis**: Can we trust the model's confidence scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add repo root to path\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Constants\n",
    "GENRE_NAMES = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "               'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "SAMPLE_RATE = 22050\n",
    "TRAIN_CHUNK_DURATION = 3.0 # Model was trained on 3s chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model and Dataset Utils\n",
    "# We try to import from modules, or fallback to running notebooks if modules aren't set up as packages\n",
    "try:\n",
    "    from model_cnn import ComplexCNN\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Model module not found; loading from notebook via %run ...\")\n",
    "    %run \"./04_model_cnn.ipynb\"\n",
    "\n",
    "try:\n",
    "    from utils.datasets_gtzan import GTZANDataset, create_dataloaders\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Dataset module not found; loading from notebook via %run ...\")\n",
    "    %run \"./01_data_loading_gtzan.ipynb\"\n",
    "\n",
    "def load_trained_model(model_path, n_classes=10, device='cpu'):\n",
    "    model = ComplexCNN(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "# Update this path to the run you want to analyze\n",
    "run_dir = Path(\"../runs/20251202_114502/\") \n",
    "model_path = run_dir / 'gtzan_cnn.pth'\n",
    "\n",
    "if not model_path.exists():\n",
    "    # Fallback to finding the latest run\n",
    "    runs_root = Path(\"../runs\")\n",
    "    all_runs = sorted([d for d in runs_root.iterdir() if d.is_dir()])\n",
    "    if all_runs:\n",
    "        run_dir = all_runs[-1]\n",
    "        model_path = run_dir / 'gtzan_cnn.pth'\n",
    "        print(f\"Specified model not found. Using latest run: {run_dir}\")\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = load_trained_model(str(model_path), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "gtzan_root = repo_root / \"data\" / \"gtzan\"\n",
    "if not gtzan_root.exists():\n",
    "    print(f\"Error: Dataset not found at {gtzan_root}\")\n",
    "else:\n",
    "    # We use the same split as training to ensure we test on unseen data\n",
    "    full_dataset = GTZANDataset(str(gtzan_root), cache_to_memory=False)\n",
    "    \n",
    "    # Create test loader with standard 3s chunks first to establish baseline\n",
    "    _, _, test_loader = create_dataloaders(\n",
    "        full_dataset, \n",
    "        batch_size=32, \n",
    "        chunk_length_sec=TRAIN_CHUNK_DURATION,\n",
    "        test_split=0.1\n",
    "    )\n",
    "    print(f\"Test set size: {len(test_loader.dataset)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db920b",
   "metadata": {},
   "source": [
    "## 1. Analysis of Short Input Sequences\n",
    "\n",
    "The model was trained on 3-second chunks. In real-world scenarios, we might have shorter clips.\n",
    "Here we simulate shorter inputs by taking the test set audio and cropping it to shorter durations (e.g., 0.5s, 1s, 2s).\n",
    "Since the model architecture (CNN) likely expects a fixed input size (corresponding to 3s), we will **pad** these shorter clips with silence to reach the 3s length.\n",
    "We hypothesize that performance will drop as the signal becomes shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_short_sequences(model, dataset, durations, device):\n",
    "    results = {}\n",
    "    \n",
    "    # We need to access the underlying file paths to re-load and crop differently\n",
    "    # The test_loader.dataset is a ChunkedDataset wrapping a Subset\n",
    "    # Let's access the subset directly\n",
    "    if hasattr(dataset, 'dataset'):\n",
    "        subset = dataset.dataset\n",
    "    else:\n",
    "        subset = dataset\n",
    "\n",
    "    print(f\"Evaluating on {len(subset)} test songs with varying durations...\")\n",
    "    \n",
    "    for duration in durations:\n",
    "        print(f\"Testing duration: {duration}s\")\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # For each song in the test subset\n",
    "        for i in range(len(subset)):\n",
    "            waveform, label = subset[i] # This returns full 30s waveform\n",
    "            \n",
    "            # Take a random chunk of 'duration' length\n",
    "            # Or take the first chunk to be deterministic\n",
    "            # Let's take the middle chunk to avoid silence at start\n",
    "            mid_point = waveform.shape[1] // 2\n",
    "            target_samples = int(duration * SAMPLE_RATE)\n",
    "            start = mid_point - target_samples // 2\n",
    "            end = start + target_samples\n",
    "            \n",
    "            chunk = waveform[:, start:end]\n",
    "            \n",
    "            # Pad to 3s (model expected input)\n",
    "            model_input_samples = int(TRAIN_CHUNK_DURATION * SAMPLE_RATE)\n",
    "            if chunk.shape[1] < model_input_samples:\n",
    "                padding = model_input_samples - chunk.shape[1]\n",
    "                # Pad at the end\n",
    "                chunk_padded = torch.nn.functional.pad(chunk, (0, padding))\n",
    "            else:\n",
    "                chunk_padded = chunk[:, :model_input_samples]\n",
    "                \n",
    "            # Add batch dim\n",
    "            input_tensor = chunk_padded.unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                pred = torch.argmax(output, dim=1).item()\n",
    "            \n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "        acc = correct / total\n",
    "        results[duration] = acc\n",
    "        print(f\"  Accuracy: {acc*100:.2f}%\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "durations_to_test = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "short_seq_results = evaluate_on_short_sequences(model, test_loader.dataset, durations_to_test, device)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(short_seq_results.keys()), [v*100 for v in short_seq_results.values()], marker='o')\n",
    "plt.title(\"Model Accuracy vs Input Duration\")\n",
    "plt.xlabel(\"Input Duration (seconds)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.axhline(y=short_seq_results[3.0]*100, color='r', linestyle='--', label='Baseline (3s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718bfa9",
   "metadata": {},
   "source": [
    "## 2. Analysis of Difficult Genres\n",
    "\n",
    "We analyze the confusion matrix to identify which genres are most frequently misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, loader, device):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "preds, labels, probs = get_predictions(model, test_loader, device)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(labels, preds, target_names=GENRE_NAMES))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=GENRE_NAMES, yticklabels=GENRE_NAMES)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Identify lowest performing genres\n",
    "class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "worst_genres_idx = np.argsort(class_acc)[:3]\n",
    "print(\"\\nTop 3 Most Difficult Genres:\")\n",
    "for idx in worst_genres_idx:\n",
    "    print(f\"{GENRE_NAMES[idx]}: {class_acc[idx]*100:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20da90",
   "metadata": {},
   "source": [
    "## 3. Confidence Analysis\n",
    "\n",
    "Do misclassified examples have lower confidence scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract confidence for correct vs incorrect predictions\n",
    "confidences = np.max(probs, axis=1)\n",
    "correct_mask = preds == labels\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(confidences[correct_mask], color='green', label='Correct', kde=True, stat=\"density\", alpha=0.5)\n",
    "sns.histplot(confidences[incorrect_mask], color='red', label='Incorrect', kde=True, stat=\"density\", alpha=0.5)\n",
    "plt.title(\"Confidence Distribution: Correct vs Incorrect Predictions\")\n",
    "plt.xlabel(\"Confidence Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Confidence (Correct): {np.mean(confidences[correct_mask]):.4f}\")\n",
    "print(f\"Average Confidence (Incorrect): {np.mean(confidences[incorrect_mask]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
