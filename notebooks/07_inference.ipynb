{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Classification Inference\n",
    "\n",
    "This notebook demonstrates how to use trained models for music genre prediction on new audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_class, model_path, n_classes, device='cpu'):\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    model = model_class(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, sample_rate=22050, duration=30):\n",
    "    \"\"\"Load and preprocess audio file for inference.\"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Pad or truncate to fixed duration\n",
    "    target_length = sample_rate * duration\n",
    "    if waveform.shape[1] > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    elif waveform.shape[1] < target_length:\n",
    "        padding = target_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Prediction (Genre Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(model, audio_path, genre_names, device='cpu', sample_rate=22050, duration=30):\n",
    "    \"\"\"Predict genre for a single audio file.\"\"\"\n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path, sample_rate, duration)\n",
    "    waveform = waveform.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(waveform)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    predicted_genre = genre_names[predicted.item()]\n",
    "    confidence_score = confidence.item()\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_k = min(5, len(genre_names))\n",
    "    top_probs, top_indices = torch.topk(probabilities[0], top_k)\n",
    "    \n",
    "    print(f\"\\nPrediction for: {Path(audio_path).name}\")\n",
    "    print(f\"Predicted Genre: {predicted_genre}\")\n",
    "    print(f\"Confidence: {confidence_score*100:.2f}%\")\n",
    "    print(f\"\\nTop {top_k} predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        print(f\"{i+1}. {genre_names[idx.item()]}: {prob.item()*100:.2f}%\")\n",
    "    \n",
    "    return predicted_genre, confidence_score, probabilities[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Prediction (Music Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, audio_path, tag_names, device='cpu', sample_rate=22050, \n",
    "                 duration=29, threshold=0.5, top_k=10):\n",
    "    \"\"\"Predict tags for a single audio file (multi-label).\"\"\"\n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path, sample_rate, duration)\n",
    "    waveform = waveform.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(waveform)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "    \n",
    "    probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    # Get tags above threshold\n",
    "    predicted_tags = [(tag_names[i], probs[i]) for i in range(len(tag_names)) \n",
    "                      if probs[i] > threshold]\n",
    "    predicted_tags.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-k tags\n",
    "    top_tags_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "    top_tags = [(tag_names[i], probs[i]) for i in top_tags_indices]\n",
    "    \n",
    "    print(f\"\\nPrediction for: {Path(audio_path).name}\")\n",
    "    print(f\"\\nPredicted tags (threshold={threshold}):\")\n",
    "    if predicted_tags:\n",
    "        for tag, prob in predicted_tags:\n",
    "            print(f\"  - {tag}: {prob*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"  No tags above threshold\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} tags:\")\n",
    "    for i, (tag, prob) in enumerate(top_tags):\n",
    "        print(f\"{i+1}. {tag}: {prob*100:.2f}%\")\n",
    "    \n",
    "    return predicted_tags, top_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(audio_path, probabilities, class_names, sample_rate=22050, save_path=None):\n",
    "    \"\"\"Visualize audio and prediction probabilities.\"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Create mel-spectrogram\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate, n_mels=128\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Plot waveform\n",
    "    axes[0].plot(waveform[0].numpy())\n",
    "    axes[0].set_title(f'Waveform - {Path(audio_path).name}')\n",
    "    axes[0].set_xlabel('Sample')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot mel-spectrogram\n",
    "    im = axes[1].imshow(mel_spec_db[0].numpy(), aspect='auto', origin='lower')\n",
    "    axes[1].set_title('Mel-Spectrogram (dB)')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Mel Frequency')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    # Plot prediction probabilities\n",
    "    axes[2].barh(range(len(class_names)), probabilities)\n",
    "    axes[2].set_yticks(range(len(class_names)))\n",
    "    axes[2].set_yticklabels(class_names)\n",
    "    axes[2].set_xlabel('Probability')\n",
    "    axes[2].set_title('Genre Predictions')\n",
    "    axes[2].set_xlim([0, 1])\n",
    "    axes[2].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch_predictions(model, data_loader, class_names, device='cpu', num_samples=9, save_path=None):\n",
    "    \"\"\"Visualize a batch of predictions with Mel-spectrograms.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    inputs, labels = next(iter(data_loader))\n",
    "    \n",
    "    # Ensure we have enough samples\n",
    "    num_samples = min(num_samples, len(inputs))\n",
    "    \n",
    "    # Select samples\n",
    "    indices = np.arange(num_samples)\n",
    "    inputs = inputs[indices].to(device)\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidences, predicted = torch.max(probabilities, 1)\n",
    "        \n",
    "    # Create grid plot\n",
    "    rows = int(np.ceil(np.sqrt(num_samples)))\n",
    "    cols = int(np.ceil(num_samples / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Compute mel-spectrogram for visualization\n",
    "        # Note: We do this manually here because the model does it internally\n",
    "        waveform = inputs[i].cpu()\n",
    "        \n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=22050, n_mels=128\n",
    "        )\n",
    "        mel_spec = mel_transform(waveform)\n",
    "        mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.imshow(mel_spec_db[0].numpy(), aspect='auto', origin='lower')\n",
    "        \n",
    "        true_label = class_names[labels[i]]\n",
    "        pred_label = class_names[predicted[i]]\n",
    "        conf = confidences[i].item()\n",
    "        \n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label} ({conf*100:.1f}%)\", color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Batch predictions saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, class_names, device='cpu', save_path=None):\n",
    "    \"\"\"Compute and plot confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Computing confusion matrix...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap='Blues', xticks_rotation=45, values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_directory(model, directory_path, genre_names, device='cpu', \n",
    "                      sample_rate=22050, duration=30):\n",
    "    \"\"\"Predict genres for all audio files in a directory.\"\"\"\n",
    "    directory = Path(directory_path)\n",
    "    audio_files = list(directory.glob('*.wav')) + list(directory.glob('*.mp3'))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(audio_files)} files from {directory_path}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        try:\n",
    "            predicted_genre, confidence, _ = predict_genre(\n",
    "                model, str(audio_file), genre_names, device, sample_rate, duration\n",
    "            )\n",
    "            results.append({\n",
    "                'file': audio_file.name,\n",
    "                'genre': predicted_genre,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_file.name}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model module not found; loading from notebook via %run ...\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_12568\\660122760.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/simple_cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_12568\\842676494.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ..\\runs\\20251127_162412\\gtzan_cnn.pth\n",
      "\n",
      "Prediction for: country.00001.wav\n",
      "Predicted Genre: country\n",
      "Confidence: 91.46%\n",
      "\n",
      "Top 5 predictions:\n",
      "1. country: 91.46%\n",
      "2. reggae: 1.21%\n",
      "3. blues: 1.17%\n",
      "4. pop: 1.05%\n",
      "5. classical: 1.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to ..\\runs\\20251127_162412\\prediction.png\n"
     ]
    }
   ],
   "source": [
    "# GTZAN genre classification\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import model (prefer module; fallback to notebook)\n",
    "try:\n",
    "    from model_cnn import ImprovedCNN\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Model module not found; loading from notebook via %run ...\")\n",
    "    %run \"./04_model_cnn.ipynb\"\n",
    "\n",
    "# Import dataset utils\n",
    "try:\n",
    "    from utils.datasets_gtzan import GTZANDataset, create_dataloaders, GENRES\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Dataset module not found; loading from notebook via %run ...\")\n",
    "    %run \"./01_data_loading_gtzan.ipynb\"\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GENRE_NAMES = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "               'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "# Update this to your latest run directory\n",
    "run_dir = Path(\"../runs/20251127_162412/\") \n",
    "model_path = run_dir / 'gtzan_cnn.pth'\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"Warning: Model not found at {model_path}. Please check the path.\")\n",
    "else:\n",
    "    # Load model\n",
    "    model = load_trained_model(\n",
    "        ImprovedCNN, \n",
    "        str(model_path), \n",
    "        n_classes=10, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 1. Single File Prediction\n",
    "    print(\"\\n--- Single File Prediction ---\")\n",
    "    audio_path = '../data/gtzan/country/country.00001.wav'\n",
    "    if Path(audio_path).exists():\n",
    "        predicted_genre, confidence, probs = predict_genre(\n",
    "            model, audio_path, GENRE_NAMES, device\n",
    "        )\n",
    "        visualize_prediction(audio_path, probs, GENRE_NAMES, save_path=str(run_dir / 'prediction_single.png'))\n",
    "    else:\n",
    "        print(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # 2. Batch Visualization & Confusion Matrix\n",
    "    print(\"\\n--- Batch Evaluation ---\")\n",
    "    # Create validation dataloader\n",
    "    gtzan_root = repo_root / \"data\" / \"gtzan\"\n",
    "    if gtzan_root.exists():\n",
    "        dataset = GTZANDataset(str(gtzan_root), cache_to_memory=False)\n",
    "        \n",
    "        # Use chunking for evaluation as well to match training\n",
    "        _, val_loader = create_dataloaders(\n",
    "            dataset, \n",
    "            batch_size=32, \n",
    "            chunk_length_sec=3.0\n",
    "        )\n",
    "        \n",
    "        # Visualize Batch Predictions\n",
    "        print(\"Visualizing batch predictions...\")\n",
    "        visualize_batch_predictions(\n",
    "            model, val_loader, GENRE_NAMES, device, \n",
    "            num_samples=16, \n",
    "            save_path=str(run_dir / 'prediction_batch.png')\n",
    "        )\n",
    "        \n",
    "        # Plot Confusion Matrix\n",
    "        plot_confusion_matrix(\n",
    "            model, val_loader, GENRE_NAMES, device, \n",
    "            save_path=str(run_dir / 'confusion_matrix.png')\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Dataset root not found: {gtzan_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Tagging Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for MTAT music tagging\n",
    "# Uncomment and adapt to your setup\n",
    "\n",
    "# from notebooks.model_cnn import DeepCNN\n",
    "\n",
    "# # Load model\n",
    "# TAG_NAMES = ['rock', 'pop', 'alternative', 'indie', ...]  # Your tag list\n",
    "# model = load_trained_model(\n",
    "#     DeepCNN,\n",
    "#     '../models/mtat_cnn.pth',\n",
    "#     n_classes=50,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# # Predict tags\n",
    "# audio_path = '../data/test_audio.wav'\n",
    "# predicted_tags, top_tags = predict_tags(\n",
    "#     model, audio_path, TAG_NAMES, device, threshold=0.3, top_k=10\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tagging-Music-Sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
