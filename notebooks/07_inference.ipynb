{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Classification Inference\n",
    "\n",
    "This notebook demonstrates how to use trained models for music genre prediction on new audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_class, model_path, n_classes, device='cpu'):\n",
    "    \"\"\"Load a trained model.\"\"\"\n",
    "    model = model_class(n_classes=n_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, sample_rate=22050, duration=30):\n",
    "    \"\"\"Load and preprocess audio file for inference.\"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Pad or truncate to fixed duration\n",
    "    target_length = sample_rate * duration\n",
    "    if waveform.shape[1] > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    elif waveform.shape[1] < target_length:\n",
    "        padding = target_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File Prediction (Genre Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(model, audio_path, genre_names, device='cpu', sample_rate=22050, duration=30):\n",
    "    \"\"\"Predict genre for a single audio file.\"\"\"\n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path, sample_rate, duration)\n",
    "    waveform = waveform.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(waveform)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    predicted_genre = genre_names[predicted.item()]\n",
    "    confidence_score = confidence.item()\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_k = min(5, len(genre_names))\n",
    "    top_probs, top_indices = torch.topk(probabilities[0], top_k)\n",
    "    \n",
    "    print(f\"\\nPrediction for: {Path(audio_path).name}\")\n",
    "    print(f\"Predicted Genre: {predicted_genre}\")\n",
    "    print(f\"Confidence: {confidence_score*100:.2f}%\")\n",
    "    print(f\"\\nTop {top_k} predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        print(f\"{i+1}. {genre_names[idx.item()]}: {prob.item()*100:.2f}%\")\n",
    "    \n",
    "    return predicted_genre, confidence_score, probabilities[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Prediction (Music Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, audio_path, tag_names, device='cpu', sample_rate=22050, \n",
    "                 duration=29, threshold=0.5, top_k=10):\n",
    "    \"\"\"Predict tags for a single audio file (multi-label).\"\"\"\n",
    "    # Preprocess audio\n",
    "    waveform = preprocess_audio(audio_path, sample_rate, duration)\n",
    "    waveform = waveform.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(waveform)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "    \n",
    "    probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    # Get tags above threshold\n",
    "    predicted_tags = [(tag_names[i], probs[i]) for i in range(len(tag_names)) \n",
    "                      if probs[i] > threshold]\n",
    "    predicted_tags.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-k tags\n",
    "    top_tags_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "    top_tags = [(tag_names[i], probs[i]) for i in top_tags_indices]\n",
    "    \n",
    "    print(f\"\\nPrediction for: {Path(audio_path).name}\")\n",
    "    print(f\"\\nPredicted tags (threshold={threshold}):\")\n",
    "    if predicted_tags:\n",
    "        for tag, prob in predicted_tags:\n",
    "            print(f\"  - {tag}: {prob*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"  No tags above threshold\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} tags:\")\n",
    "    for i, (tag, prob) in enumerate(top_tags):\n",
    "        print(f\"{i+1}. {tag}: {prob*100:.2f}%\")\n",
    "    \n",
    "    return predicted_tags, top_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(audio_path, probabilities, class_names, sample_rate=22050, save_path=None):\n",
    "    \"\"\"Visualize audio and prediction probabilities.\"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Create mel-spectrogram\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate, n_mels=128\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Plot waveform\n",
    "    axes[0].plot(waveform[0].numpy())\n",
    "    axes[0].set_title(f'Waveform - {Path(audio_path).name}')\n",
    "    axes[0].set_xlabel('Sample')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot mel-spectrogram\n",
    "    im = axes[1].imshow(mel_spec_db[0].numpy(), aspect='auto', origin='lower')\n",
    "    axes[1].set_title('Mel-Spectrogram (dB)')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Mel Frequency')\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    # Plot prediction probabilities\n",
    "    axes[2].barh(range(len(class_names)), probabilities)\n",
    "    axes[2].set_yticks(range(len(class_names)))\n",
    "    axes[2].set_yticklabels(class_names)\n",
    "    axes[2].set_xlabel('Probability')\n",
    "    axes[2].set_title('Genre Predictions')\n",
    "    axes[2].set_xlim([0, 1])\n",
    "    axes[2].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_directory(model, directory_path, genre_names, device='cpu', \n",
    "                      sample_rate=22050, duration=30):\n",
    "    \"\"\"Predict genres for all audio files in a directory.\"\"\"\n",
    "    directory = Path(directory_path)\n",
    "    audio_files = list(directory.glob('*.wav')) + list(directory.glob('*.mp3'))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(audio_files)} files from {directory_path}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        try:\n",
    "            predicted_genre, confidence, _ = predict_genre(\n",
    "                model, str(audio_file), genre_names, device, sample_rate, duration\n",
    "            )\n",
    "            results.append({\n",
    "                'file': audio_file.name,\n",
    "                'genre': predicted_genre,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_file.name}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model module not found; loading from notebook via %run ...\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n",
      "Model loaded from ../models/simple_cnn.pth\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n",
      "Model loaded from ../models/simple_cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_16476\\660122760.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n",
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_16476\\842676494.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ..\\runs\\20251127_143203\\gtzan_cnn.pth\n",
      "\n",
      "Prediction for: country.00001.wav\n",
      "Predicted Genre: country\n",
      "Confidence: 64.64%\n",
      "\n",
      "Top 5 predictions:\n",
      "1. country: 64.64%\n",
      "2. rock: 14.99%\n",
      "3. pop: 8.02%\n",
      "4. blues: 5.25%\n",
      "5. jazz: 4.57%\n",
      "\n",
      "Prediction for: country.00001.wav\n",
      "Predicted Genre: country\n",
      "Confidence: 64.64%\n",
      "\n",
      "Top 5 predictions:\n",
      "1. country: 64.64%\n",
      "2. rock: 14.99%\n",
      "3. pop: 8.02%\n",
      "4. blues: 5.25%\n",
      "5. jazz: 4.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mar20\\Desktop\\Natural Language Processing\\Tagging-Music-Sequences\\.venv\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to ..\\runs\\20251127_143203\\prediction.png\n"
     ]
    }
   ],
   "source": [
    "# GTZAN genre classification\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import model (prefer module; fallback to notebook)\n",
    "try:\n",
    "    from model_cnn import SimpleCNN\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Model module not found; loading from notebook via %run ...\")\n",
    "    %run \"./04_model_cnn.ipynb\"\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "GENRE_NAMES = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "               'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "run_dir = Path(\"../runs/20251127_150146/\")\n",
    "\n",
    "# Load model\n",
    "model = load_trained_model(\n",
    "    SimpleCNN, \n",
    "    str(run_dir / 'gtzan_cnn.pth'), \n",
    "    n_classes=10, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Predict single file\n",
    "audio_path = '../data/gtzan/country/country.00001.wav'\n",
    "predicted_genre, confidence, probs = predict_genre(\n",
    "    model, audio_path, GENRE_NAMES, device\n",
    ")\n",
    "\n",
    "# Visualize and save\n",
    "visualize_prediction(audio_path, probs, GENRE_NAMES, save_path=str(run_dir / 'prediction.png'))\n",
    "print(f\"Prediction saved to {run_dir / 'prediction.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Tagging Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for MTAT music tagging\n",
    "# Uncomment and adapt to your setup\n",
    "\n",
    "# from notebooks.model_cnn import DeepCNN\n",
    "\n",
    "# # Load model\n",
    "# TAG_NAMES = ['rock', 'pop', 'alternative', 'indie', ...]  # Your tag list\n",
    "# model = load_trained_model(\n",
    "#     DeepCNN,\n",
    "#     '../models/mtat_cnn.pth',\n",
    "#     n_classes=50,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# # Predict tags\n",
    "# audio_path = '../data/test_audio.wav'\n",
    "# predicted_tags, top_tags = predict_tags(\n",
    "#     model, audio_path, TAG_NAMES, device, threshold=0.3, top_k=10\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tagging-Music-Sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
