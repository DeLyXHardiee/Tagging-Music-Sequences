{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Music Classification Models\n",
    "\n",
    "This notebook demonstrates how to train the CNN and RNN models for music classification and tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Single-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, \n",
    "                save_path='../models/best_model.pth'):\n",
    "    \"\"\"Complete training loop with early stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
    "                                                       patience=5, verbose=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Multi-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multilabel(model, train_loader, val_loader, num_epochs, learning_rate, device,\n",
    "                     save_path='../models/best_model_multilabel.pth'):\n",
    "    \"\"\"Training loop for multi-label classification.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer (BCE for multi-label)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                                                       patience=5, verbose=True)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc='Validation')\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, multi_label=False):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 if not multi_label else 1, figsize=(15, 5))\n",
    "    \n",
    "    if not multi_label:\n",
    "        # Loss plot\n",
    "        axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "        axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(history['train_acc'], label='Train Accuracy')\n",
    "        axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy (%)')\n",
    "        axes[1].set_title('Training and Validation Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "    else:\n",
    "        # Loss plot only for multi-label\n",
    "        axes.plot(history['train_loss'], label='Train Loss')\n",
    "        axes.plot(history['val_loss'], label='Val Loss')\n",
    "        axes.set_xlabel('Epoch')\n",
    "        axes.set_ylabel('Loss')\n",
    "        axes.set_title('Training and Validation Loss')\n",
    "        axes.legend()\n",
    "        axes.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### For Single-label Classification (GTZAN, FMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train SimpleCNN on GTZAN\n",
    "# Uncomment and adapt to your dataset\n",
    "\n",
    "# from notebooks.model_cnn import SimpleCNN\n",
    "# from notebooks.data_loading_gtzan import GTZANDataset, create_dataloaders\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = GTZANDataset(GTZAN_PATH)\n",
    "# train_loader, val_loader = create_dataloaders(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # Create model\n",
    "# model = SimpleCNN(n_classes=10)\n",
    "\n",
    "# # Train\n",
    "# history = train_model(\n",
    "#     model, train_loader, val_loader, \n",
    "#     num_epochs=NUM_EPOCHS, \n",
    "#     learning_rate=LEARNING_RATE, \n",
    "#     device=device,\n",
    "#     save_path='../models/gtzan_cnn.pth'\n",
    "# )\n",
    "\n",
    "# # Plot results\n",
    "# plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Multi-label Classification (MTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train DeepCNN on MTAT\n",
    "# Uncomment and adapt to your dataset\n",
    "\n",
    "# from notebooks.model_cnn import DeepCNN\n",
    "# from notebooks.data_loading_mtat import MTATDataset, create_dataloaders\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = MTATDataset(MTAT_AUDIO_PATH, MTAT_ANNOTATIONS_PATH, top_tags=50)\n",
    "# train_loader, val_loader = create_dataloaders(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # Create model\n",
    "# model = DeepCNN(n_classes=50)\n",
    "\n",
    "# # Train\n",
    "# history = train_multilabel(\n",
    "#     model, train_loader, val_loader,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     device=device,\n",
    "#     save_path='../models/mtat_cnn.pth'\n",
    "# )\n",
    "\n",
    "# # Plot results\n",
    "# plot_training_history(history, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, genre_names=None):\n",
    "    \"\"\"Evaluate model and print detailed metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return all_preds, all_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
