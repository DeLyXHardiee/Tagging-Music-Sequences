{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Music Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001  # Lowered from 0.001\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory created at: ..\\runs\\20251127_172211\n"
     ]
    }
   ],
   "source": [
    "# Setup run directory\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = Path(f\"../runs/{run_id}\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Run directory created at: {run_dir}\")\n",
    "\n",
    "# Create changes.md\n",
    "changes_file = run_dir / \"changes.md\"\n",
    "with open(changes_file, \"w\") as f:\n",
    "    f.write(f\"# Run {run_id}\\n\\n\")\n",
    "    f.write(\"## Configuration\\n\")\n",
    "    f.write(f\"- Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"- Learning Rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"- Epochs: {NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"- Device: {device}\\n\")\n",
    "    f.write(f\"- Data Strategy: Chunking (3s chunks, 50% overlap)\\n\")\n",
    "    f.write(f\"- Augmentation: Noise=0.01, Shift=0.3\\n\")\n",
    "    f.write(f\"- Optimization: In-memory caching + Mixed Precision (AMP)\\n\")\n",
    "    f.write(f\"- Stability: Seed=42, Weight Decay=1e-4 (Standard), Gradient Clipping=1.0\\n\")\n",
    "    f.write(f\"- Data Split: Stratified (Balanced Validation Set)\\n\\n\")\n",
    "    f.write(\"## Changes\\n\")\n",
    "    f.write(\"- Added more dropout layers to the ImprovedCNN model to reduce overfitting and removed layer 4 in the model.\\n\\n\")\n",
    "    f.write(\"## Results\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Single-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, \n",
    "                save_path='../models/best_model.pth', changes_file=None):\n",
    "    \"\"\"Complete training loop with early stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Reduced weight decay back to 1e-4 as we have more data now\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n",
    "                                                       patience=5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Val Loss: {history['val_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Train Acc: {history['train_acc'][-1]:.2f}%\\n\")\n",
    "            f.write(f\"- Final Val Acc: {history['val_acc'][-1]:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Multi-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multilabel(model, train_loader, val_loader, num_epochs, learning_rate, device,\n",
    "                     save_path='../models/best_model_multilabel.pth'):\n",
    "    \"\"\"Training loop for multi-label classification.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer (BCE for multi-label)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                                                       patience=5)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc='Validation')\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, multi_label=False, save_path=None):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 if not multi_label else 1, figsize=(15, 5))\n",
    "    \n",
    "    if not multi_label:\n",
    "        # Loss plot\n",
    "        axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "        axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(history['train_acc'], label='Train Accuracy')\n",
    "        axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy (%)')\n",
    "        axes[1].set_title('Training and Validation Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "    else:\n",
    "        # Loss plot only for multi-label\n",
    "        axes.plot(history['train_loss'], label='Train Loss')\n",
    "        axes.plot(history['val_loss'], label='Val Loss')\n",
    "        axes.set_xlabel('Epoch')\n",
    "        axes.set_ylabel('Loss')\n",
    "        axes.set_title('Training and Validation Loss')\n",
    "        axes.legend()\n",
    "        axes.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Single-label Classification (GTZAN, FMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model module not found; loading from notebook via %run ...\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n",
      "Model loaded from ../models/simple_cnn.pth\n",
      "Caching 999 audio files to memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_2828\\660122760.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching complete.\n",
      "GTZAN files: 999\n",
      "Created stratified split: 799 train songs, 200 val songs\n",
      "Applying chunking: 3.0s chunks with 50% overlap\n",
      "Chunked dataset sizes: 15181 train chunks, 3800 val chunks\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:26<00:00, 17.84it/s, loss=1.25, acc=41.9]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.83it/s, loss=0.966, acc=45.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7765, Train Acc: 41.95%\n",
      "Val Loss: 2.0315, Val Acc: 45.63%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.69it/s, loss=2.05, acc=59.9]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.13it/s, loss=1.27, acc=50.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4327, Train Acc: 59.88%\n",
      "Val Loss: 2.4362, Val Acc: 50.37%\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.16it/s, loss=1.64, acc=67.3] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 54.77it/s, loss=1.09, acc=55.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2853, Train Acc: 67.30%\n",
      "Val Loss: 1.9425, Val Acc: 55.82%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.01it/s, loss=1.14, acc=71.1] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.93it/s, loss=1.93, acc=57.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2058, Train Acc: 71.08%\n",
      "Val Loss: 1.7753, Val Acc: 57.95%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.24it/s, loss=1.41, acc=74]   \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 58.69it/s, loss=0.991, acc=61.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1501, Train Acc: 74.01%\n",
      "Val Loss: 1.5527, Val Acc: 61.13%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.30it/s, loss=1.43, acc=76.2] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 58.17it/s, loss=0.973, acc=65.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0971, Train Acc: 76.19%\n",
      "Val Loss: 1.5536, Val Acc: 65.16%\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.36it/s, loss=1.21, acc=78.5] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 59.01it/s, loss=1.04, acc=55.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0524, Train Acc: 78.48%\n",
      "Val Loss: 1.9042, Val Acc: 55.68%\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.49it/s, loss=0.767, acc=80.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.76it/s, loss=0.897, acc=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0164, Train Acc: 80.38%\n",
      "Val Loss: 1.4780, Val Acc: 64.11%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.86it/s, loss=0.75, acc=81]   \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 59.07it/s, loss=0.972, acc=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9944, Train Acc: 80.98%\n",
      "Val Loss: 1.5963, Val Acc: 64.13%\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:18<00:00, 25.27it/s, loss=1.09, acc=82.1] \n",
      "Validation: 100%|██████████| 119/119 [00:01<00:00, 59.57it/s, loss=0.955, acc=59.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9680, Train Acc: 82.14%\n",
      "Val Loss: 1.6747, Val Acc: 59.55%\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 23.76it/s, loss=0.812, acc=83.9]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.96it/s, loss=1.2, acc=65.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9386, Train Acc: 83.87%\n",
      "Val Loss: 1.6148, Val Acc: 65.21%\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:21<00:00, 22.36it/s, loss=0.951, acc=84]  \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.53it/s, loss=0.767, acc=66.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9287, Train Acc: 84.00%\n",
      "Val Loss: 1.4748, Val Acc: 66.89%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.00it/s, loss=0.907, acc=84.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 49.15it/s, loss=0.898, acc=59.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9153, Train Acc: 84.38%\n",
      "Val Loss: 1.8421, Val Acc: 59.92%\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:27<00:00, 17.11it/s, loss=1.05, acc=86]   \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 49.09it/s, loss=0.645, acc=66.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8895, Train Acc: 86.00%\n",
      "Val Loss: 1.5461, Val Acc: 66.05%\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:22<00:00, 21.47it/s, loss=0.752, acc=86.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.35it/s, loss=0.796, acc=65.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8767, Train Acc: 86.54%\n",
      "Val Loss: 1.5468, Val Acc: 65.95%\n",
      "\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:21<00:00, 22.36it/s, loss=0.825, acc=87.9]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 51.17it/s, loss=1, acc=69.1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8511, Train Acc: 87.88%\n",
      "Val Loss: 1.3974, Val Acc: 69.08%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:21<00:00, 22.00it/s, loss=0.875, acc=88.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 49.96it/s, loss=0.98, acc=68.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8381, Train Acc: 88.37%\n",
      "Val Loss: 1.3941, Val Acc: 68.08%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:22<00:00, 21.38it/s, loss=0.668, acc=89.2]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.11it/s, loss=1.12, acc=69.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8233, Train Acc: 89.22%\n",
      "Val Loss: 1.4522, Val Acc: 69.53%\n",
      "\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.15it/s, loss=1.25, acc=89.9] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 54.03it/s, loss=0.771, acc=72.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8093, Train Acc: 89.87%\n",
      "Val Loss: 1.2582, Val Acc: 72.37%\n",
      "✓ Model saved to ..\\runs\\20251127_172211\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 22.63it/s, loss=0.799, acc=90.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.44it/s, loss=0.673, acc=69.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7988, Train Acc: 90.08%\n",
      "Val Loss: 1.3164, Val Acc: 69.76%\n",
      "\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:22<00:00, 20.97it/s, loss=0.995, acc=90.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 50.65it/s, loss=1.74, acc=68.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7943, Train Acc: 90.46%\n",
      "Val Loss: 1.4173, Val Acc: 68.47%\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:26<00:00, 18.01it/s, loss=0.947, acc=91.3]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 47.89it/s, loss=1.24, acc=68.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7775, Train Acc: 91.32%\n",
      "Val Loss: 1.4323, Val Acc: 68.42%\n",
      "\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:23<00:00, 20.33it/s, loss=0.722, acc=91.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 51.20it/s, loss=0.922, acc=70]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7719, Train Acc: 91.56%\n",
      "Val Loss: 1.3573, Val Acc: 69.97%\n",
      "\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:26<00:00, 18.18it/s, loss=0.811, acc=92]  \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.85it/s, loss=0.672, acc=67.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7632, Train Acc: 91.95%\n",
      "Val Loss: 1.3820, Val Acc: 67.63%\n",
      "\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:24<00:00, 19.11it/s, loss=0.681, acc=92.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 47.68it/s, loss=1.23, acc=66.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7501, Train Acc: 92.64%\n",
      "Val Loss: 1.4749, Val Acc: 66.58%\n",
      "\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:27<00:00, 17.26it/s, loss=0.909, acc=93.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.49it/s, loss=1.42, acc=66.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7202, Train Acc: 93.74%\n",
      "Val Loss: 1.5118, Val Acc: 66.89%\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.33it/s, loss=0.682, acc=94.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.47it/s, loss=1.24, acc=67]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7119, Train Acc: 94.11%\n",
      "Val Loss: 1.4534, Val Acc: 67.03%\n",
      "\n",
      "Epoch 28/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.44it/s, loss=1.03, acc=94.2] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.93it/s, loss=1.13, acc=70.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7145, Train Acc: 94.25%\n",
      "Val Loss: 1.3126, Val Acc: 70.68%\n",
      "\n",
      "Epoch 29/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.60it/s, loss=0.748, acc=94.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.38it/s, loss=1.3, acc=67.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7059, Train Acc: 94.64%\n",
      "Val Loss: 1.4643, Val Acc: 67.53%\n",
      "\n",
      "Early stopping triggered after 29 epochs\n"
     ]
    }
   ],
   "source": [
    "# Train SimpleCNN on GTZAN\n",
    "\n",
    "# Ensure repository root is on sys.path\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import model (prefer module; fallback to notebook)\n",
    "try:\n",
    "    from model_cnn import ImprovedCNN\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Model module not found; loading from notebook via %run ...\")\n",
    "    %run \"./04_model_cnn.ipynb\"\n",
    "\n",
    "# Import dataset from stable utils module (Windows-safe)\n",
    "try:\n",
    "    from utils.datasets_gtzan import GTZANDataset, create_dataloaders, GENRES, AudioAugmentation\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Dataset module not found; loading from notebook via %run ...\")\n",
    "    %run \"./01_data_loading_gtzan.ipynb\"\n",
    "\n",
    "# Create dataset with in-memory caching\n",
    "gtzan_root = repo_root / \"data\" / \"gtzan\"\n",
    "dataset = GTZANDataset(str(gtzan_root), cache_to_memory=True)\n",
    "print(f\"GTZAN files: {len(dataset)}\")\n",
    "\n",
    "# Define augmentation\n",
    "train_transform = AudioAugmentation(noise_level=0.01, shift_max=0.3)\n",
    "\n",
    "# Create loaders with Stratified Split AND Chunking\n",
    "# NOTE: With cache_to_memory=True, we must use num_workers=0 on Windows to avoid \n",
    "# pickling the entire cached dataset to worker processes, which causes hangs/OOM.\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    train_transform=train_transform,\n",
    "    chunk_length_sec=3.0 # Enable chunking\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = ImprovedCNN(n_classes=10)\n",
    "\n",
    "# Train\n",
    "history = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device,\n",
    "    save_path=str(run_dir / 'gtzan_cnn.pth'),\n",
    "    changes_file=changes_file\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plot_training_history(history, save_path=str(run_dir / 'training_history.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Multi-label Classification (MTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train DeepCNN on MTAT\n",
    "# Uncomment and adapt to your dataset\n",
    "\n",
    "# from notebooks.model_cnn import DeepCNN\n",
    "# from notebooks.data_loading_mtat import MTATDataset, create_dataloaders\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = MTATDataset(MTAT_AUDIO_PATH, MTAT_ANNOTATIONS_PATH, top_tags=50)\n",
    "# train_loader, val_loader = create_dataloaders(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # Create model\n",
    "# model = DeepCNN(n_classes=50)\n",
    "\n",
    "# # Train\n",
    "# history = train_multilabel(\n",
    "#     model, train_loader, val_loader,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     device=device,\n",
    "#     save_path='../models/mtat_cnn.pth'\n",
    "# )\n",
    "\n",
    "# # Plot results\n",
    "# plot_training_history(history, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, genre_names=None, changes_file=None):\n",
    "    \"\"\"Evaluate model and print detailed metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- Test Accuracy: {accuracy*100:.2f}%\\n\")\n",
    "            f.write(f\"- Test Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"- Test Recall: {recall:.4f}\\n\")\n",
    "            f.write(f\"- Test F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "evaluate_model(\n",
    "    model, val_loader, device, genre_names=GENRES, changes_file=changes_file\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tagging-Music-Sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
