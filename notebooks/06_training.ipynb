{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Music Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001  # Increased back to 0.001 for OneCycleLR\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 15 # Increased patience for OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory created at: ..\\runs\\20251129_115258\n"
     ]
    }
   ],
   "source": [
    "# Setup run directory\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = Path(f\"../runs/{run_id}\")\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Run directory created at: {run_dir}\")\n",
    "\n",
    "# Create changes.md\n",
    "changes_file = run_dir / \"changes.md\"\n",
    "with open(changes_file, \"w\") as f:\n",
    "    f.write(f\"# Run {run_id}\\n\\n\")\n",
    "    f.write(\"## Configuration\\n\")\n",
    "    f.write(f\"- Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"- Learning Rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"- Epochs: {NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"- Device: {device}\\n\")\n",
    "    f.write(f\"- Data Strategy: Chunking (3s chunks, 50% overlap)\\n\")\n",
    "    f.write(f\"- Augmentation: Noise=0.01, Shift=0.3\\n\")\n",
    "    f.write(f\"- Optimization: In-memory caching + Mixed Precision (AMP)\\n\")\n",
    "    f.write(f\"- Stability: Seed=42, Weight Decay=1e-4 (Standard), Gradient Clipping=1.0\\n\")\n",
    "    f.write(f\"- Data Split: Stratified (Balanced Validation Set)\\n\\n\")\n",
    "    f.write(\"## Changes\\n\")\n",
    "    f.write(\"- Increased model capacity by restoring the 4th residual layer and double the channel depth (up to 512 channels)\\n\")\n",
    "    f.write(\"- Updated scheduler to use OneCycleLR (SOTA for CNN models). It starts with a low learning rate, ramps up to a high one, and then anneals down to near zero.\\n\")\n",
    "    f.write(\"- Switched from Adam optimizer to AdamW with higher weight decay (0.01). AdamW decouples weight decay from the gradient update, which usually leads to better generalization.\\n\\n\")\n",
    "    f.write(\"## Results\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Single-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch with Mixup.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.4, device=device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Weighted accuracy for mixup\n",
    "        correct += (lam * (predicted == targets_a).float() + (1 - lam) * (predicted == targets_b).float()).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, \n",
    "                save_path='../models/best_model.pth', changes_file=None):\n",
    "    \"\"\"Complete training loop with OneCycleLR.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01) # Increased weight decay for AdamW\n",
    "    \n",
    "    # OneCycleLR Scheduler\n",
    "    # Steps per epoch is len(train_loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate, \n",
    "        steps_per_epoch=len(train_loader), \n",
    "        epochs=num_epochs,\n",
    "        pct_start=0.3, # Warmup for 30% of training\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=1000.0\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Apply Mixup\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.4, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Step scheduler every batch for OneCycleLR\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (lam * (predicted == targets_a).float() + (1 - lam) * (predicted == targets_b).float()).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Val Loss: {history['val_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"- Final Train Acc: {history['train_acc'][-1]:.2f}%\\n\")\n",
    "            f.write(f\"- Final Val Acc: {history['val_acc'][-1]:.2f}%\\n\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function (Multi-label Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multilabel(model, train_loader, val_loader, num_epochs, learning_rate, device,\n",
    "                     save_path='../models/best_model_multilabel.pth'):\n",
    "    \"\"\"Training loop for multi-label classification.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer (BCE for multi-label)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n",
    "                                                       patience=5)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= train_batches\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc='Validation')\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss /= val_batches\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✓ Model saved to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, multi_label=False, save_path=None):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2 if not multi_label else 1, figsize=(15, 5))\n",
    "    \n",
    "    if not multi_label:\n",
    "        # Loss plot\n",
    "        axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "        axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(history['train_acc'], label='Train Accuracy')\n",
    "        axes[1].plot(history['val_acc'], label='Val Accuracy')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy (%)')\n",
    "        axes[1].set_title('Training and Validation Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "    else:\n",
    "        # Loss plot only for multi-label\n",
    "        axes.plot(history['train_loss'], label='Train Loss')\n",
    "        axes.plot(history['val_loss'], label='Val Loss')\n",
    "        axes.set_xlabel('Epoch')\n",
    "        axes.set_ylabel('Loss')\n",
    "        axes.set_title('Training and Validation Loss')\n",
    "        axes.legend()\n",
    "        axes.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Single-label Classification (GTZAN, FMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model module not found; loading from notebook via %run ...\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "SimpleCNN:\n",
      "SimpleCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 422986\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 10])\n",
      "DeepCNN:\n",
      "DeepCNN(\n",
      "  (mel_spec): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (amplitude_to_db): AmplitudeToDB()\n",
      "  (conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=50, bias=True)\n",
      ")\n",
      "\n",
      "Number of parameters: 4858738\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n",
      "\n",
      "Input shape: torch.Size([4, 1, 661500])\n",
      "Output shape: torch.Size([4, 50])\n",
      "Model saved to ../models/simple_cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mar20\\AppData\\Local\\Temp\\ipykernel_17196\\660122760.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/simple_cnn.pth\n",
      "Caching 999 audio files to memory...\n",
      "Caching 999 audio files to memory...\n",
      "Caching complete.\n",
      "GTZAN files: 999\n",
      "Caching complete.\n",
      "GTZAN files: 999\n",
      "Created stratified split: 799 train songs, 200 val songs\n",
      "Applying chunking: 3.0s chunks with 50% overlap\n",
      "Chunked dataset sizes: 15181 train chunks, 3800 val chunks\n",
      "Created stratified split: 799 train songs, 200 val songs\n",
      "Applying chunking: 3.0s chunks with 50% overlap\n",
      "Chunked dataset sizes: 15181 train chunks, 3800 val chunks\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:45<00:00, 10.47it/s, loss=1.37, acc=34.5]\n",
      "Training: 100%|██████████| 475/475 [00:45<00:00, 10.47it/s, loss=1.37, acc=34.5]\n",
      "Validation: 100%|██████████| 119/119 [00:04<00:00, 24.33it/s, loss=0.898, acc=52.7]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9576, Train Acc: 34.48%\n",
      "Val Loss: 1.6428, Val Acc: 52.68%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:37<00:00, 12.75it/s, loss=1.92, acc=48.4]\n",
      "Training: 100%|██████████| 475/475 [00:37<00:00, 12.75it/s, loss=1.92, acc=48.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.80it/s, loss=1.18, acc=57.3] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7140, Train Acc: 48.36%\n",
      "Val Loss: 1.6281, Val Acc: 57.32%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.33it/s, loss=1.47, acc=54.3]\n",
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.33it/s, loss=1.47, acc=54.3]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 44.35it/s, loss=0.921, acc=59.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 44.35it/s, loss=0.921, acc=59.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6109, Train Acc: 54.29%\n",
      "Val Loss: 1.5184, Val Acc: 59.58%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.90it/s, loss=1.15, acc=57.1]\n",
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.90it/s, loss=1.15, acc=57.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 39.90it/s, loss=1.15, acc=60.8] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5664, Train Acc: 57.05%\n",
      "Val Loss: 1.5163, Val Acc: 60.82%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.07it/s, loss=0.991, acc=59.4]\n",
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.07it/s, loss=0.991, acc=59.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 42.42it/s, loss=0.816, acc=61.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 42.42it/s, loss=0.816, acc=61.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5234, Train Acc: 59.42%\n",
      "Val Loss: 1.4741, Val Acc: 61.47%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.43it/s, loss=1.87, acc=61.8] \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.43it/s, loss=1.87, acc=61.8]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.37it/s, loss=1.05, acc=68]   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4792, Train Acc: 61.79%\n",
      "Val Loss: 1.2929, Val Acc: 67.97%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:42<00:00, 11.29it/s, loss=1.88, acc=62.7] \n",
      "Training: 100%|██████████| 475/475 [00:42<00:00, 11.29it/s, loss=1.88, acc=62.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.36it/s, loss=0.904, acc=66.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.36it/s, loss=0.904, acc=66.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4636, Train Acc: 62.67%\n",
      "Val Loss: 1.4380, Val Acc: 66.05%\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:37<00:00, 12.70it/s, loss=1.4, acc=64]    \n",
      "Training: 100%|██████████| 475/475 [00:37<00:00, 12.70it/s, loss=1.4, acc=64] \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 36.72it/s, loss=1.05, acc=65.4] \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 36.72it/s, loss=1.05, acc=65.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4344, Train Acc: 64.05%\n",
      "Val Loss: 1.3535, Val Acc: 65.42%\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.24it/s, loss=1.81, acc=64]   \n",
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.24it/s, loss=1.81, acc=64]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 35.38it/s, loss=0.851, acc=72.2]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4428, Train Acc: 64.01%\n",
      "Val Loss: 1.2138, Val Acc: 72.24%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:31<00:00, 14.94it/s, loss=1.18, acc=65.1] \n",
      "Training: 100%|██████████| 475/475 [00:31<00:00, 14.94it/s, loss=1.18, acc=65.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.21it/s, loss=0.849, acc=67.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.21it/s, loss=0.849, acc=67.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4249, Train Acc: 65.06%\n",
      "Val Loss: 1.2954, Val Acc: 67.53%\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:35<00:00, 13.41it/s, loss=1.57, acc=67.3] \n",
      "Training: 100%|██████████| 475/475 [00:35<00:00, 13.41it/s, loss=1.57, acc=67.3]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.21it/s, loss=0.702, acc=65.5]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.21it/s, loss=0.702, acc=65.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3738, Train Acc: 67.27%\n",
      "Val Loss: 1.3326, Val Acc: 65.47%\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.42it/s, loss=1.4, acc=67.9]  \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.42it/s, loss=1.4, acc=67.9] \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.78it/s, loss=0.843, acc=73.7]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3690, Train Acc: 67.95%\n",
      "Val Loss: 1.1839, Val Acc: 73.68%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.96it/s, loss=1.6, acc=68.1]  \n",
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.96it/s, loss=1.6, acc=68.1]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 39.02it/s, loss=0.866, acc=66]  \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 39.02it/s, loss=0.866, acc=66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3582, Train Acc: 68.15%\n",
      "Val Loss: 1.3186, Val Acc: 65.97%\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.74it/s, loss=1.39, acc=68.6] \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.74it/s, loss=1.39, acc=68.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 42.69it/s, loss=0.839, acc=73.2]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 42.69it/s, loss=0.839, acc=73.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3558, Train Acc: 68.58%\n",
      "Val Loss: 1.1900, Val Acc: 73.16%\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.80it/s, loss=1.07, acc=69.6] \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.80it/s, loss=1.07, acc=69.6]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.27it/s, loss=0.892, acc=72.2]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.27it/s, loss=0.892, acc=72.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3334, Train Acc: 69.63%\n",
      "Val Loss: 1.2131, Val Acc: 72.18%\n",
      "\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.40it/s, loss=1.46, acc=68.7] \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.40it/s, loss=1.46, acc=68.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.60it/s, loss=1.3, acc=67.7]  \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.60it/s, loss=1.3, acc=67.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3557, Train Acc: 68.66%\n",
      "Val Loss: 1.3148, Val Acc: 67.66%\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.71it/s, loss=1.89, acc=70.7] \n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.71it/s, loss=1.89, acc=70.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 43.43it/s, loss=0.75, acc=73.6] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 43.43it/s, loss=0.75, acc=73.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3096, Train Acc: 70.67%\n",
      "Val Loss: 1.2068, Val Acc: 73.58%\n",
      "\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.85it/s, loss=0.954, acc=70.7]\n",
      "Training: 100%|██████████| 475/475 [00:34<00:00, 13.85it/s, loss=0.954, acc=70.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.54it/s, loss=1.18, acc=69.4] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 41.54it/s, loss=1.18, acc=69.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3154, Train Acc: 70.72%\n",
      "Val Loss: 1.3183, Val Acc: 69.39%\n",
      "\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:30<00:00, 15.36it/s, loss=1.99, acc=72.2] \n",
      "Training: 100%|██████████| 475/475 [00:30<00:00, 15.36it/s, loss=1.99, acc=72.2]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.32it/s, loss=1.04, acc=74]   \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 37.32it/s, loss=1.04, acc=74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2840, Train Acc: 72.24%\n",
      "Val Loss: 1.1894, Val Acc: 74.03%\n",
      "\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.48it/s, loss=0.694, acc=73.2]\n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.48it/s, loss=0.694, acc=73.2]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 39.08it/s, loss=0.838, acc=72.3]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2589, Train Acc: 73.18%\n",
      "Val Loss: 1.1738, Val Acc: 72.32%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:36<00:00, 12.98it/s, loss=1.07, acc=72.8] \n",
      "Training: 100%|██████████| 475/475 [00:36<00:00, 12.98it/s, loss=1.07, acc=72.8]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 32.74it/s, loss=0.711, acc=76.1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2723, Train Acc: 72.79%\n",
      "Val Loss: 1.1449, Val Acc: 76.11%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:30<00:00, 15.32it/s, loss=1.01, acc=73.8] \n",
      "Training: 100%|██████████| 475/475 [00:30<00:00, 15.32it/s, loss=1.01, acc=73.8] \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 36.73it/s, loss=0.834, acc=75]  \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 36.73it/s, loss=0.834, acc=75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2507, Train Acc: 73.80%\n",
      "Val Loss: 1.1747, Val Acc: 74.97%\n",
      "\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.70it/s, loss=0.723, acc=71.5]\n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.70it/s, loss=0.723, acc=71.5]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 33.76it/s, loss=0.891, acc=76.6]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 33.76it/s, loss=0.891, acc=76.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2960, Train Acc: 71.50%\n",
      "Val Loss: 1.1460, Val Acc: 76.55%\n",
      "\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:39<00:00, 12.14it/s, loss=0.945, acc=74.4]\n",
      "Training: 100%|██████████| 475/475 [00:39<00:00, 12.14it/s, loss=0.945, acc=74.4]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 33.72it/s, loss=0.795, acc=76.7]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 33.72it/s, loss=0.795, acc=76.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2404, Train Acc: 74.38%\n",
      "Val Loss: 1.1503, Val Acc: 76.66%\n",
      "\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.31it/s, loss=1.04, acc=74.6] \n",
      "Training: 100%|██████████| 475/475 [00:33<00:00, 14.31it/s, loss=1.04, acc=74.6] \n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 39.41it/s, loss=0.875, acc=75.4]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 39.41it/s, loss=0.875, acc=75.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2374, Train Acc: 74.62%\n",
      "Val Loss: 1.1703, Val Acc: 75.39%\n",
      "\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:35<00:00, 13.28it/s, loss=1.42, acc=73.9] \n",
      "Training: 100%|██████████| 475/475 [00:35<00:00, 13.28it/s, loss=1.42, acc=73.9]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 38.96it/s, loss=0.835, acc=76.1]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 38.96it/s, loss=0.835, acc=76.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2500, Train Acc: 73.90%\n",
      "Val Loss: 1.1527, Val Acc: 76.13%\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.81it/s, loss=0.894, acc=73.6]\n",
      "Training: 100%|██████████| 475/475 [00:32<00:00, 14.81it/s, loss=0.894, acc=73.6]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 30.01it/s, loss=0.782, acc=75.9]\n",
      "Validation: 100%|██████████| 119/119 [00:03<00:00, 30.01it/s, loss=0.782, acc=75.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2555, Train Acc: 73.61%\n",
      "Val Loss: 1.1834, Val Acc: 75.87%\n",
      "\n",
      "Epoch 28/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:31<00:00, 14.89it/s, loss=1.29, acc=76]   \n",
      "Training: 100%|██████████| 475/475 [00:31<00:00, 14.89it/s, loss=1.29, acc=76] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.52it/s, loss=0.851, acc=76.3]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 53.52it/s, loss=0.851, acc=76.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2056, Train Acc: 76.03%\n",
      "Val Loss: 1.1269, Val Acc: 76.34%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 29/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 22.64it/s, loss=1.28, acc=76.9] \n",
      "Training: 100%|██████████| 475/475 [00:20<00:00, 22.64it/s, loss=1.28, acc=76.9] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.65it/s, loss=0.8, acc=78.1]  \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.65it/s, loss=0.8, acc=78.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1888, Train Acc: 76.87%\n",
      "Val Loss: 1.1169, Val Acc: 78.11%\n",
      "✓ Model saved to ..\\runs\\20251129_115258\\gtzan_cnn.pth\n",
      "\n",
      "Epoch 30/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.13it/s, loss=0.745, acc=75.4]\n",
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.13it/s, loss=0.745, acc=75.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.83it/s, loss=0.742, acc=76.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.83it/s, loss=0.742, acc=76.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2188, Train Acc: 75.37%\n",
      "Val Loss: 1.1660, Val Acc: 76.13%\n",
      "\n",
      "Epoch 31/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.18it/s, loss=1.31, acc=75.2] \n",
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.18it/s, loss=1.31, acc=75.2]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.61it/s, loss=0.765, acc=77.4]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 56.61it/s, loss=0.765, acc=77.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2189, Train Acc: 75.18%\n",
      "Val Loss: 1.1298, Val Acc: 77.45%\n",
      "\n",
      "Epoch 32/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.38it/s, loss=0.975, acc=74.8]\n",
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.38it/s, loss=0.975, acc=74.8]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 57.08it/s, loss=0.744, acc=77.8]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 57.08it/s, loss=0.744, acc=77.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2299, Train Acc: 74.83%\n",
      "Val Loss: 1.1285, Val Acc: 77.84%\n",
      "\n",
      "Epoch 33/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.31it/s, loss=0.843, acc=75.8]\n",
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.31it/s, loss=0.843, acc=75.8]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.80it/s, loss=0.847, acc=76.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 55.80it/s, loss=0.847, acc=76.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2076, Train Acc: 75.84%\n",
      "Val Loss: 1.1526, Val Acc: 76.61%\n",
      "\n",
      "Epoch 34/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.25it/s, loss=1.65, acc=74.9] \n",
      "Training: 100%|██████████| 475/475 [00:19<00:00, 24.25it/s, loss=1.65, acc=74.9] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 57.65it/s, loss=0.859, acc=77.2]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 57.65it/s, loss=0.859, acc=77.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2214, Train Acc: 74.92%\n",
      "Val Loss: 1.1303, Val Acc: 77.18%\n",
      "\n",
      "Epoch 35/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.55it/s, loss=1.53, acc=76]   \n",
      "Training: 100%|██████████| 475/475 [00:20<00:00, 23.55it/s, loss=1.53, acc=76]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.08it/s, loss=0.778, acc=76.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.08it/s, loss=0.778, acc=76.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2100, Train Acc: 76.00%\n",
      "Val Loss: 1.1483, Val Acc: 76.50%\n",
      "\n",
      "Epoch 36/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:20<00:00, 22.85it/s, loss=1.1, acc=77.5]  \n",
      "Training: 100%|██████████| 475/475 [00:20<00:00, 22.85it/s, loss=1.1, acc=77.5]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 49.78it/s, loss=0.813, acc=77.1]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 49.78it/s, loss=0.813, acc=77.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1752, Train Acc: 77.53%\n",
      "Val Loss: 1.1382, Val Acc: 77.13%\n",
      "\n",
      "Epoch 37/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:23<00:00, 20.17it/s, loss=0.8, acc=76]    \n",
      "Training: 100%|██████████| 475/475 [00:23<00:00, 20.17it/s, loss=0.8, acc=76] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.04it/s, loss=0.839, acc=76.6]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.04it/s, loss=0.839, acc=76.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2015, Train Acc: 75.96%\n",
      "Val Loss: 1.1351, Val Acc: 76.63%\n",
      "\n",
      "Epoch 38/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:22<00:00, 21.27it/s, loss=1.37, acc=75.7] \n",
      "Training: 100%|██████████| 475/475 [00:22<00:00, 21.27it/s, loss=1.37, acc=75.7]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 51.42it/s, loss=0.74, acc=77.3] \n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 51.42it/s, loss=0.74, acc=77.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2058, Train Acc: 75.71%\n",
      "Val Loss: 1.1461, Val Acc: 77.29%\n",
      "\n",
      "Epoch 39/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 475/475 [00:25<00:00, 18.42it/s, loss=1.43, acc=75.3] \n",
      "Training: 100%|██████████| 475/475 [00:25<00:00, 18.42it/s, loss=1.43, acc=75.3]\n",
      "Validation: 100%|██████████| 119/119 [00:02<00:00, 48.59it/s, loss=0.869, acc=76]  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2174, Train Acc: 75.31%\n",
      "Val Loss: 1.1539, Val Acc: 76.03%\n",
      "\n",
      "Early stopping triggered after 39 epochs\n"
     ]
    }
   ],
   "source": [
    "# Train SimpleCNN on GTZAN\n",
    "\n",
    "# Ensure repository root is on sys.path\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import model (prefer module; fallback to notebook)\n",
    "try:\n",
    "    from model_cnn import ImprovedCNN\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Model module not found; loading from notebook via %run ...\")\n",
    "    %run \"./04_model_cnn.ipynb\"\n",
    "\n",
    "# Import dataset from stable utils module (Windows-safe)\n",
    "try:\n",
    "    from utils.datasets_gtzan import GTZANDataset, create_dataloaders, GENRES, AudioAugmentation\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Dataset module not found; loading from notebook via %run ...\")\n",
    "    %run \"./01_data_loading_gtzan.ipynb\"\n",
    "\n",
    "# Create dataset with in-memory caching\n",
    "gtzan_root = repo_root / \"data\" / \"gtzan\"\n",
    "dataset = GTZANDataset(str(gtzan_root), cache_to_memory=True)\n",
    "print(f\"GTZAN files: {len(dataset)}\")\n",
    "\n",
    "# Define augmentation\n",
    "train_transform = AudioAugmentation(noise_level=0.01, shift_max=0.3)\n",
    "\n",
    "# Create loaders with Stratified Split AND Chunking\n",
    "# NOTE: With cache_to_memory=True, we must use num_workers=0 on Windows to avoid \n",
    "# pickling the entire cached dataset to worker processes, which causes hangs/OOM.\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    train_transform=train_transform,\n",
    "    chunk_length_sec=3.0, # Enable chunking\n",
    "    test_split=0.1 # Create test split\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = ImprovedCNN(n_classes=10)\n",
    "\n",
    "# Train\n",
    "history = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device,\n",
    "    save_path=str(run_dir / 'gtzan_cnn.pth'),\n",
    "    changes_file=changes_file\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plot_training_history(history, save_path=str(run_dir / 'training_history.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Multi-label Classification (MTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train DeepCNN on MTAT\n",
    "# Uncomment and adapt to your dataset\n",
    "\n",
    "# from notebooks.model_cnn import DeepCNN\n",
    "# from notebooks.data_loading_mtat import MTATDataset, create_dataloaders\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = MTATDataset(MTAT_AUDIO_PATH, MTAT_ANNOTATIONS_PATH, top_tags=50)\n",
    "# train_loader, val_loader = create_dataloaders(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # Create model\n",
    "# model = DeepCNN(n_classes=50)\n",
    "\n",
    "# # Train\n",
    "# history = train_multilabel(\n",
    "#     model, train_loader, val_loader,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     device=device,\n",
    "#     save_path='../models/mtat_cnn.pth'\n",
    "# )\n",
    "\n",
    "# # Plot results\n",
    "# plot_training_history(history, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 119/119 [00:02<00:00, 54.62it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics (Chunk-Level):\n",
      "Accuracy: 76.03%\n",
      "Precision: 0.7805\n",
      "Recall: 0.7603\n",
      "F1-Score: 0.7583\n",
      "\n",
      "Evaluating on 200 songs (aggregating 19 chunks each)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Song Eval: 100%|██████████| 200/200 [00:02<00:00, 93.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song-Level Accuracy: 80.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device, genre_names=None, changes_file=None, split_name=\"Test\"):\n",
    "    \"\"\"Evaluate model and print detailed metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f'Evaluating {split_name}'):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{split_name} Metrics (Chunk-Level):\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- {split_name} Accuracy (Chunk): {accuracy*100:.2f}%\\n\")\n",
    "            f.write(f\"- {split_name} Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"- {split_name} Recall: {recall:.4f}\\n\")\n",
    "            f.write(f\"- {split_name} F1-Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "def evaluate_by_song(model, val_dataset, device, changes_file=None, split_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy by aggregating chunk predictions for each song.\n",
    "    Assumes val_dataset is ordered by song.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_songs = 0\n",
    "    \n",
    "    # Check if dataset has num_chunks (ChunkedDataset)\n",
    "    if not hasattr(val_dataset, 'num_chunks'):\n",
    "        print(\"Dataset does not appear to be a ChunkedDataset. Skipping song-level evaluation.\")\n",
    "        return 0.0\n",
    "\n",
    "    num_chunks = val_dataset.num_chunks\n",
    "    total_songs = len(val_dataset) // num_chunks\n",
    "    \n",
    "    print(f\"\\nEvaluating on {total_songs} songs ({split_name}) (aggregating {num_chunks} chunks each)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(total_songs), desc='Song Eval'):\n",
    "            # Get all chunks for this song\n",
    "            chunks = []\n",
    "            label = None\n",
    "            \n",
    "            # Indices for this song's chunks\n",
    "            start_idx = i * num_chunks\n",
    "            \n",
    "            for j in range(num_chunks):\n",
    "                c, l = val_dataset[start_idx + j]\n",
    "                chunks.append(c)\n",
    "                label = l \n",
    "            \n",
    "            # Stack: (num_chunks, channels, time)\n",
    "            chunks_tensor = torch.stack(chunks).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            outputs = model(chunks_tensor) # (num_chunks, n_classes)\n",
    "            \n",
    "            # Soft Voting: Average probabilities (logits are fine for argmax)\n",
    "            avg_output = torch.mean(outputs, dim=0)\n",
    "            pred_label = torch.argmax(avg_output).item()\n",
    "            \n",
    "            if pred_label == label:\n",
    "                correct_songs += 1\n",
    "                \n",
    "    song_acc = 100 * correct_songs / total_songs\n",
    "    print(f\"{split_name} Song-Level Accuracy: {song_acc:.2f}%\")\n",
    "    \n",
    "    if changes_file:\n",
    "        with open(changes_file, \"a\") as f:\n",
    "            f.write(f\"- {split_name} Song-Level Accuracy: {song_acc:.2f}%\\n\")\n",
    "            \n",
    "    return song_acc\n",
    "\n",
    "# Run evaluations on Validation Set\n",
    "with open(changes_file, \"a\") as f:\n",
    "    f.write(\"\\n--- Validation Set ---\\n\")\n",
    "print(\"\\n--- Validation Set Evaluation ---\")\n",
    "evaluate_model(\n",
    "    model, val_loader, device, genre_names=GENRES, changes_file=changes_file, split_name=\"Validation\"\n",
    ")\n",
    "evaluate_by_song(model, val_loader.dataset, device, changes_file=changes_file, split_name=\"Validation\")\n",
    "\n",
    "# Run evaluations on Test Set\n",
    "with open(changes_file, \"a\") as f:\n",
    "    f.write(\"\\n--- Test Set ---\\n\")\n",
    "print(\"\\n--- Test Set Evaluation ---\")\n",
    "evaluate_model(\n",
    "    model, test_loader, device, genre_names=GENRES, changes_file=changes_file, split_name=\"Test\"\n",
    ")\n",
    "\n",
    "evaluate_by_song(model, test_loader.dataset, device, changes_file=changes_file, split_name=\"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tagging-Music-Sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
